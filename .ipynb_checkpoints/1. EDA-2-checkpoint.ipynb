{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "In this notebook we will perform any data processing that is required in order to be able to load the provided data into a Pandas based dataframe. Then we will take a quick look at each feature with the intention of discovering superficial patterns that hopefully will aid and inspire us for modeling.\n",
    "\n",
    "***The answer to the first question can be found here: ***\n",
    "- [Answer to first question](#cell_first_question)\n",
    "\n",
    "\n",
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [15, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load our data into json format and then parse it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_json('data/original/city_search.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20022 entries, 0 to 20021\n",
      "Data columns (total 4 columns):\n",
      "session_id        20022 non-null object\n",
      "unix_timestamp    20022 non-null object\n",
      "cities            20022 non-null object\n",
      "user              20022 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 625.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations***\n",
    "- We have 20022 non-null rows.\n",
    "- The data type for each column is object. This is incorrect since we know that we have dates and numbers in our data.\n",
    "\n",
    "Let's have a look at the first few lines to verify this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>cities</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[X061RFWB06K9V]</td>\n",
       "      <td>[1442503708]</td>\n",
       "      <td>[New York NY, Newark NJ]</td>\n",
       "      <td>[[{'user_id': 2024, 'joining_date': '2015-03-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[5AZ2X2A9BHH5U]</td>\n",
       "      <td>[1441353991]</td>\n",
       "      <td>[New York NY, Jersey City NJ, Philadelphia PA]</td>\n",
       "      <td>[[{'user_id': 2853, 'joining_date': '2015-03-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[SHTB4IYAX4PX6]</td>\n",
       "      <td>[1440843490]</td>\n",
       "      <td>[San Antonio TX]</td>\n",
       "      <td>[[{'user_id': 10958, 'joining_date': '2015-03-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[JBRB8MZGTX3M4]</td>\n",
       "      <td>[1427268063]</td>\n",
       "      <td>[Edmonton AB]</td>\n",
       "      <td>[[{'user_id': 7693, 'joining_date': '2015-03-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[YJCMPURC2FL9C]</td>\n",
       "      <td>[1430559067]</td>\n",
       "      <td>[Phoenix AZ, Houston TX]</td>\n",
       "      <td>[[{'user_id': 7506, 'joining_date': '2015-02-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id unix_timestamp  \\\n",
       "0  [X061RFWB06K9V]   [1442503708]   \n",
       "1  [5AZ2X2A9BHH5U]   [1441353991]   \n",
       "2  [SHTB4IYAX4PX6]   [1440843490]   \n",
       "3  [JBRB8MZGTX3M4]   [1427268063]   \n",
       "4  [YJCMPURC2FL9C]   [1430559067]   \n",
       "\n",
       "                                           cities  \\\n",
       "0                        [New York NY, Newark NJ]   \n",
       "1  [New York NY, Jersey City NJ, Philadelphia PA]   \n",
       "2                                [San Antonio TX]   \n",
       "3                                   [Edmonton AB]   \n",
       "4                        [Phoenix AZ, Houston TX]   \n",
       "\n",
       "                                                user  \n",
       "0  [[{'user_id': 2024, 'joining_date': '2015-03-2...  \n",
       "1  [[{'user_id': 2853, 'joining_date': '2015-03-2...  \n",
       "2  [[{'user_id': 10958, 'joining_date': '2015-03-...  \n",
       "3  [[{'user_id': 7693, 'joining_date': '2015-03-1...  \n",
       "4  [[{'user_id': 7506, 'joining_date': '2015-02-2...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe contains json formatted strings inside its cells. Let's open the file as json and then parse it into a pandas dataframe. This will allow us to flatten the data and therefore be able to access it and analyze it with our tools of preference (pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open json file into a dictionary\n",
    "with open('data/original/city_search.json', 'r') as f:\n",
    "    city_search_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_id': ['X061RFWB06K9V'],\n",
       " 'unix_timestamp': [1442503708],\n",
       " 'cities': ['New York NY, Newark NJ'],\n",
       " 'user': [[{'user_id': 2024, 'joining_date': '2015-03-22', 'country': 'UK'}]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_search_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will transform the dictionary that we created from the json above into a pandas dataframe\n",
    "def create_dataframe(city_search_dict):\n",
    "    df_flat = pd.DataFrame(columns =  ['session_id', 'unix_timestamp', 'cities', 'user_id', 'joining_date', 'country'])  \n",
    "    for idx, search in enumerate(city_search_dict):\n",
    "        for usr_det in search['user'][0]:\n",
    "            user_id = usr_det['user_id']\n",
    "            joining_date = usr_det['joining_date']\n",
    "            country = usr_det['country']\n",
    "        \n",
    "    df_flat.loc[idx] = [search['session_id'][0], search['unix_timestamp'][0], search['cities'][0], user_id, joining_date, country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the method to create the datafrma from dictionary if it hasn't been done before. Otherwise load the already\n",
    "# created file from csv to dataframe\n",
    "\n",
    "if path.exists('data/flattened/df_city_search.csv') == False:\n",
    "    # Transform the dictionary into a pandas df\n",
    "    df_flat = create_dataframe(city_search_dict)\n",
    "    # Save it to disk\n",
    "    df_flat.to_csv('data/flattened/df_city_search.csv', index = None, header=True)\n",
    "else:\n",
    "    df_flat = pd.read_csv('data/flattened/df_city_search.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now finished processing our data so we can explore it with python. From now on as we execute this notebook we won't preprocess it every time, instead we will load it from the place we saved it at the first time we process it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>cities</th>\n",
       "      <th>user_id</th>\n",
       "      <th>joining_date</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X061RFWB06K9V</td>\n",
       "      <td>1442503708</td>\n",
       "      <td>New York NY, Newark NJ</td>\n",
       "      <td>2024</td>\n",
       "      <td>2015-03-22</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5AZ2X2A9BHH5U</td>\n",
       "      <td>1441353991</td>\n",
       "      <td>New York NY, Jersey City NJ, Philadelphia PA</td>\n",
       "      <td>2853</td>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHTB4IYAX4PX6</td>\n",
       "      <td>1440843490</td>\n",
       "      <td>San Antonio TX</td>\n",
       "      <td>10958</td>\n",
       "      <td>2015-03-06</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JBRB8MZGTX3M4</td>\n",
       "      <td>1427268063</td>\n",
       "      <td>Edmonton AB</td>\n",
       "      <td>7693</td>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YJCMPURC2FL9C</td>\n",
       "      <td>1430559067</td>\n",
       "      <td>Phoenix AZ, Houston TX</td>\n",
       "      <td>7506</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LKOKP80QD6BEO</td>\n",
       "      <td>1434199991</td>\n",
       "      <td>San Diego CA</td>\n",
       "      <td>3743</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>ES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>YOVUIM79SGS5Y</td>\n",
       "      <td>1443171887</td>\n",
       "      <td>Montreal QC, Chicago IL</td>\n",
       "      <td>8831</td>\n",
       "      <td>2015-03-02</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SHXEY67QWKP9K</td>\n",
       "      <td>1431766104</td>\n",
       "      <td>Calgary AB, New York NY</td>\n",
       "      <td>587</td>\n",
       "      <td>2015-03-16</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9P74JL5KMC9G9</td>\n",
       "      <td>1434816246</td>\n",
       "      <td>Chicago IL, New York NY</td>\n",
       "      <td>365</td>\n",
       "      <td>2015-03-22</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UY49RTVRX3GL7</td>\n",
       "      <td>1439104667</td>\n",
       "      <td>New York NY</td>\n",
       "      <td>5995</td>\n",
       "      <td>2015-03-21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  unix_timestamp  \\\n",
       "0  X061RFWB06K9V      1442503708   \n",
       "1  5AZ2X2A9BHH5U      1441353991   \n",
       "2  SHTB4IYAX4PX6      1440843490   \n",
       "3  JBRB8MZGTX3M4      1427268063   \n",
       "4  YJCMPURC2FL9C      1430559067   \n",
       "5  LKOKP80QD6BEO      1434199991   \n",
       "6  YOVUIM79SGS5Y      1443171887   \n",
       "7  SHXEY67QWKP9K      1431766104   \n",
       "8  9P74JL5KMC9G9      1434816246   \n",
       "9  UY49RTVRX3GL7      1439104667   \n",
       "\n",
       "                                         cities  user_id joining_date country  \n",
       "0                        New York NY, Newark NJ     2024   2015-03-22      UK  \n",
       "1  New York NY, Jersey City NJ, Philadelphia PA     2853   2015-03-28      DE  \n",
       "2                                San Antonio TX    10958   2015-03-06      UK  \n",
       "3                                   Edmonton AB     7693   2015-03-12      IT  \n",
       "4                        Phoenix AZ, Houston TX     7506   2015-02-28      UK  \n",
       "5                                  San Diego CA     3743   2015-03-04      ES  \n",
       "6                       Montreal QC, Chicago IL     8831   2015-03-02          \n",
       "7                       Calgary AB, New York NY      587   2015-03-16          \n",
       "8                       Chicago IL, New York NY      365   2015-03-22      US  \n",
       "9                                   New York NY     5995   2015-03-21          "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that Nan is encoded as ''. Otherwise the code that follows might not work as expected\n",
    "df_flat.replace(np.nan, '', regex=True, inplace=True)\n",
    "df_flat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the data analysis part of the project. We will start by studying the features one by one to see what insights naturally bubble up to the surface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space of possible cities\n",
    "\n",
    "Let's now extract the cities from the data to create a list of all the possible cities. This will help us tokenize our city sequences in order to train sequential models down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['New York NY, Newark NJ',\n",
       "       'New York NY, Jersey City NJ, Philadelphia PA', 'San Antonio TX',\n",
       "       ..., 'Edmonton AB, Houston TX, Toronto ON, Los Angeles CA',\n",
       "       'San Diego CA, New York NY, Houston TX',\n",
       "       'Houston TX, Chicago IL, Los Angeles CA'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['cities'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cities(dataframe):\n",
    "    city_set = set()\n",
    "    for index, row in dataframe.iterrows():\n",
    "        searched_cities = [city.strip() for city in row['cities'].split(\",\")]\n",
    "\n",
    "        for city in searched_cities:\n",
    "            city_set.add(city)\n",
    "    city_list = list(city_set)\n",
    "    return city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anaheim CA', 'Arlington TX', 'Atlanta GA', 'Austin TX', 'Bakersfield CA', 'Baltimore MD', 'Birmingham AL', 'Boston MA', 'Buffalo NY', 'Calgary AB', 'Chandler AZ', 'Charlotte NC', 'Chesapeake VA', 'Chicago IL', 'Cincinnati OH', 'Cleveland OH', 'Columbus OH', 'Corpus Christi TX', 'Dallas TX', 'Detroit MI', 'Edmonton AB', 'Fort Wayne IN', 'Fort Worth TX', 'Fresno CA', 'Glendale AZ', 'Greensboro NC', 'Halifax NS', 'Hamilton ON', 'Hialeah FL', 'Houston TX', 'Indianapolis IN', 'Jacksonville FL', 'Jersey City NJ', 'Kansas City MO', 'Kitchener ON', 'Lexington KY', 'Lincoln NE', 'London ON', 'Long Beach CA', 'Los Angeles CA', 'Louisville KY', 'Madison WI', 'Memphis TN', 'Mesa AZ', 'Miami FL', 'Milwaukee WI', 'Minneapolis MN', 'Montreal QC', 'Nashville TN', 'New Orleans LA', 'New York NY', 'Newark NJ', 'Norfolk VA', 'OTTAWA ON', 'Oakland CA', 'Oklahoma City OK', 'Omaha NE', 'Oshawa ON', 'Philadelphia PA', 'Phoenix AZ', 'Pittsburgh PA', 'Plano TX', 'Portland OR', 'Quebec QC', 'Raleigh NC', 'Riverside CA', 'Sacramento CA', 'Saint Catharines-Niagara ON', 'Saint Paul MN', 'Saint Petersburg FL', 'San Antonio TX', 'San Diego CA', 'San Francisco CA', 'San Jose CA', 'Santa Ana CA', 'Scottsdale AZ', 'Seattle WA', 'Stockton CA', 'Tampa FL', 'Toledo OH', 'Toronto ON', 'Tucson AZ', 'Tulsa OK', 'Vancouver BC', 'Victoria BC', 'Virginia Beach VA', 'WASHINGTON DC', 'Wichita KS', 'Windsor ON']\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# Convert set to list\n",
    "city_list = extract_cities(df_flat)\n",
    "# Sort cities in alphabetical orther for reproducibility purposes.\n",
    "city_list.sort()\n",
    "\n",
    "print(city_list)\n",
    "print(len(city_list))\n",
    "\n",
    "with open('data/city_list.json', 'w') as json_file:\n",
    "    json.dump({'city_list':city_list}, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with 89 different cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Let's take a look at our features independently, maybe we will discover some interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the ids to strings in the dataframe so they are seen as categorical variables by pandas\n",
    "df_flat['user_id'] = df_flat.apply(lambda row: str(row['user_id']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: cities_searched\n",
    "\n",
    "Let's create a new feature that will describe the number of cities per search. This will make our task of exploring city related features easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat['cities_searched'] = df_flat.apply(lambda row: row['cities'].count(',')+1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>cities</th>\n",
       "      <th>user_id</th>\n",
       "      <th>joining_date</th>\n",
       "      <th>country</th>\n",
       "      <th>cities_searched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X061RFWB06K9V</td>\n",
       "      <td>1442503708</td>\n",
       "      <td>New York NY, Newark NJ</td>\n",
       "      <td>2024</td>\n",
       "      <td>2015-03-22</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5AZ2X2A9BHH5U</td>\n",
       "      <td>1441353991</td>\n",
       "      <td>New York NY, Jersey City NJ, Philadelphia PA</td>\n",
       "      <td>2853</td>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>DE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHTB4IYAX4PX6</td>\n",
       "      <td>1440843490</td>\n",
       "      <td>San Antonio TX</td>\n",
       "      <td>10958</td>\n",
       "      <td>2015-03-06</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JBRB8MZGTX3M4</td>\n",
       "      <td>1427268063</td>\n",
       "      <td>Edmonton AB</td>\n",
       "      <td>7693</td>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>IT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YJCMPURC2FL9C</td>\n",
       "      <td>1430559067</td>\n",
       "      <td>Phoenix AZ, Houston TX</td>\n",
       "      <td>7506</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  unix_timestamp  \\\n",
       "0  X061RFWB06K9V      1442503708   \n",
       "1  5AZ2X2A9BHH5U      1441353991   \n",
       "2  SHTB4IYAX4PX6      1440843490   \n",
       "3  JBRB8MZGTX3M4      1427268063   \n",
       "4  YJCMPURC2FL9C      1430559067   \n",
       "\n",
       "                                         cities user_id joining_date country  \\\n",
       "0                        New York NY, Newark NJ    2024   2015-03-22      UK   \n",
       "1  New York NY, Jersey City NJ, Philadelphia PA    2853   2015-03-28      DE   \n",
       "2                                San Antonio TX   10958   2015-03-06      UK   \n",
       "3                                   Edmonton AB    7693   2015-03-12      IT   \n",
       "4                        Phoenix AZ, Houston TX    7506   2015-02-28      UK   \n",
       "\n",
       "   cities_searched  \n",
       "0                2  \n",
       "1                3  \n",
       "2                1  \n",
       "3                1  \n",
       "4                2  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20022.000000\n",
       "mean         1.648986\n",
       "std          0.874272\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max         11.000000\n",
       "Name: cities_searched, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['cities_searched'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: tokenized sequences\n",
    "\n",
    "Let's create a feature that will encode the search sequences as lists of integers (we'll call them tokens). This feature will be of crucial importance since we will use it directly to represent one search session which corresponds to each datapoint. It will also be used to pass into our models in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sequence(sequence_str):\n",
    "    raw_sequence = [city.strip() for city in sequence_str.split(\",\")]\n",
    "    token_sequence = []\n",
    "    for city in raw_sequence:\n",
    "        token_sequence.append(city_list.index(city)+1)\n",
    "    \n",
    "    return token_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the search sequences\n",
    "df_flat['city_integer_sequence'] = df_flat['cities'].apply(tokenize_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original search: New York NY, Newark NJ\n",
      "Tokenized search: [51, 52]\n"
     ]
    }
   ],
   "source": [
    "print('Original search: ' + str(df_flat['cities'].values[0]))\n",
    "print('Tokenized search: ' + str(df_flat['city_integer_sequence'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: binary sequences\n",
    "\n",
    "Let's also create a binary representation of each search so that the value of the token of each city searched is positionaly encoded as one in a vector of zeros of length total number of cities. Look at the logs bellow for clarity.\n",
    "This feature will help us to characterize our data by patterns in the binary sequences that represent each search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_sequence(sequence_str):\n",
    "    raw_sequence = [city.strip() for city in sequence_str.split(\",\")]\n",
    "    integer_sequence = [0] * len(city_list)\n",
    "    \n",
    "    for city in raw_sequence:\n",
    "        integer_sequence[city_list.index(city)] += 1\n",
    "        \n",
    "    return integer_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat['binary_sequence'] = df_flat['cities'].apply(binarize_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original search: New York NY, Newark NJ\n",
      "Tokenized search: [51, 52]\n",
      "Binarized search: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('Original search: ' + str(df_flat['cities'].values[0]))\n",
    "print('Tokenized search: ' + str(df_flat['city_integer_sequence'].values[0]))\n",
    "print('Binarized search: ' + str(df_flat['binary_sequence'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering : unix_timestamp\n",
    "\n",
    "Let's create a new feature called 'timestamp' from 'unix_timestamp'. This feature will be of great use since it will allow us to see our data as a time series over the time range defined by the dates in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                   20022\n",
       "unique                  20003\n",
       "top       2015-09-25 08:44:47\n",
       "freq                        2\n",
       "first     2015-02-28 03:40:35\n",
       "last      2015-10-02 00:48:22\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['timestamp'] = df_flat.apply(lambda row:pd.to_datetime( row['unix_timestamp'], unit = 's'), axis=1)\n",
    "df_flat['timestamp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_flat['joining_date'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                   20022\n",
       "unique                     35\n",
       "top       2015-03-02 00:00:00\n",
       "freq                      857\n",
       "first     2015-02-28 00:00:00\n",
       "last      2015-04-03 00:00:00\n",
       "Name: joining_date, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['joining_date'] = pd.to_datetime(df_flat['joining_date'],infer_datetime_format=True)\n",
    "df_flat['joining_date'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering : country_code\n",
    "\n",
    "Let's create a feature that provides a numerical encoding for country so that we can visualize our more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the missing country is encoded as 0\n",
    "country_color_dict = {'': 0, 'UK':1, 'DE':2, 'IT':3, 'ES':4, 'US':5, 'FR':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat['country_code'] = df_flat.apply (lambda row: country_color_dict[row['country']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>unix_timestamp</th>\n",
       "      <th>cities</th>\n",
       "      <th>user_id</th>\n",
       "      <th>joining_date</th>\n",
       "      <th>country</th>\n",
       "      <th>cities_searched</th>\n",
       "      <th>city_integer_sequence</th>\n",
       "      <th>binary_sequence</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X061RFWB06K9V</td>\n",
       "      <td>1442503708</td>\n",
       "      <td>New York NY, Newark NJ</td>\n",
       "      <td>2024</td>\n",
       "      <td>2015-03-22</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "      <td>[51, 52]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2015-09-17 15:28:28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5AZ2X2A9BHH5U</td>\n",
       "      <td>1441353991</td>\n",
       "      <td>New York NY, Jersey City NJ, Philadelphia PA</td>\n",
       "      <td>2853</td>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>DE</td>\n",
       "      <td>3</td>\n",
       "      <td>[51, 33, 59]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2015-09-04 08:06:31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHTB4IYAX4PX6</td>\n",
       "      <td>1440843490</td>\n",
       "      <td>San Antonio TX</td>\n",
       "      <td>10958</td>\n",
       "      <td>2015-03-06</td>\n",
       "      <td>UK</td>\n",
       "      <td>1</td>\n",
       "      <td>[71]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2015-08-29 10:18:10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JBRB8MZGTX3M4</td>\n",
       "      <td>1427268063</td>\n",
       "      <td>Edmonton AB</td>\n",
       "      <td>7693</td>\n",
       "      <td>2015-03-12</td>\n",
       "      <td>IT</td>\n",
       "      <td>1</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2015-03-25 07:21:03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YJCMPURC2FL9C</td>\n",
       "      <td>1430559067</td>\n",
       "      <td>Phoenix AZ, Houston TX</td>\n",
       "      <td>7506</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>UK</td>\n",
       "      <td>2</td>\n",
       "      <td>[60, 30]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2015-05-02 09:31:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  unix_timestamp  \\\n",
       "0  X061RFWB06K9V      1442503708   \n",
       "1  5AZ2X2A9BHH5U      1441353991   \n",
       "2  SHTB4IYAX4PX6      1440843490   \n",
       "3  JBRB8MZGTX3M4      1427268063   \n",
       "4  YJCMPURC2FL9C      1430559067   \n",
       "\n",
       "                                         cities user_id joining_date country  \\\n",
       "0                        New York NY, Newark NJ    2024   2015-03-22      UK   \n",
       "1  New York NY, Jersey City NJ, Philadelphia PA    2853   2015-03-28      DE   \n",
       "2                                San Antonio TX   10958   2015-03-06      UK   \n",
       "3                                   Edmonton AB    7693   2015-03-12      IT   \n",
       "4                        Phoenix AZ, Houston TX    7506   2015-02-28      UK   \n",
       "\n",
       "   cities_searched city_integer_sequence  \\\n",
       "0                2              [51, 52]   \n",
       "1                3          [51, 33, 59]   \n",
       "2                1                  [71]   \n",
       "3                1                  [21]   \n",
       "4                2              [60, 30]   \n",
       "\n",
       "                                     binary_sequence           timestamp  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 2015-09-17 15:28:28   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 2015-09-04 08:06:31   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 2015-08-29 10:18:10   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 2015-03-25 07:21:03   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... 2015-05-02 09:31:07   \n",
       "\n",
       "   country_code  \n",
       "0             1  \n",
       "1             2  \n",
       "2             1  \n",
       "3             3  \n",
       "4             1  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     20022\n",
       "unique     1329\n",
       "top        [51]\n",
       "freq       2278\n",
       "Name: city_integer_sequence, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat['city_integer_sequence'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_flat.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets group our data per user_id and country to see if we can find any relationships between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', '10019'), ('', '1002'), ('', '10029')]\n",
      "['', 'DE', 'ES']\n"
     ]
    }
   ],
   "source": [
    "df_grouped_userid_and_country = df_flat.groupby(['country', 'user_id'])\n",
    "print(list(df_grouped_userid_and_country.groups.keys())[:3])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import groupby\n",
    "\n",
    "# def count_ocurrence_of_one_feature_into_another(dataframe, base_feature, secondary_feature ):\n",
    "#     # List of countries with the unknown one as the first one\n",
    "#     unique_countries = dataframe[base_feature].unique()\n",
    "#     df_grouped = dataframe.groupby([base_feature, secondary_feature])\n",
    "    \n",
    "#     key_list = list(df_grouped.groups.keys())\n",
    "#     # Group the secondary_feature values that belong to each value in base_feature\n",
    "#     user_ids_per_country = {c: set() for c in unique_countries}\n",
    "\n",
    "#     for key in key_list:\n",
    "#         user_ids_per_country[key[0]].add(key[1])\n",
    "        \n",
    "#     return user_ids_per_country\n",
    "\n",
    "# def analyze_groups(dataframe, base_feature):\n",
    "#     df_grouped_country = dataframe.groupby(base_feature)\n",
    "    \n",
    "#     # See how secondary feature distributes over base_feature by taking each value in the later.\n",
    "#     for key, group in df_grouped_country:\n",
    "#         all_records = sorted(list(group['user_id'].values), reverse=True) \n",
    "#         if all_records:\n",
    "#             print('Using *{}* to group {}'.format(key, secondary_feature))\n",
    "#             # Create dictionary of repetitions of each element keyd by the feature value\n",
    "#             repetitions_dict = {key : len(list(group)) for key, group in groupby(all_records)}\n",
    "#             sorted_list_max = sorted(repetitions_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#             sorted_list_min = sorted(repetitions_dict.items(), key=lambda x: x[1], reverse=False)\n",
    "#             unique_records = set(all_records)\n",
    "\n",
    "#             print('Total records {}'.format(len(all_records)))\n",
    "#             print('Most repeted elements: {}'.format(sorted_list_max[:5]))\n",
    "#             print('Least repeted elements: {}'.format(sorted_list_min[:5]))\n",
    "#             print('Unique records {}'.format(len(unique_records)))\n",
    "            \n",
    "#         else:\n",
    "#             print('No records found')\n",
    "#         print('-------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Base feature *country* with 7 unique values: ['UK' 'DE' 'IT' 'ES' '' 'US' 'FR']\n",
      "-----------------------------------------------------------------\n",
      "Using ** to group user_id\n",
      "Total records 2820\n",
      "Most repeted elements: [('8440', 9), ('7793', 9), ('7203', 9), ('336', 9), ('8816', 8)]\n",
      "Least repeted elements: [('9936', 1), ('9896', 1), ('9629', 1), ('9586', 1), ('9458', 1)]\n",
      "Unique records 792\n",
      "-------------\n",
      "Using *DE* to group user_id\n",
      "Total records 3638\n",
      "Most repeted elements: [('4404', 12), ('7363', 10), ('6508', 10), ('2300', 10), ('9929', 9)]\n",
      "Least repeted elements: [('979', 1), ('9767', 1), ('9766', 1), ('9727', 1), ('97', 1)]\n",
      "Unique records 1051\n",
      "-------------\n",
      "Using *ES* to group user_id\n",
      "Total records 1953\n",
      "Most repeted elements: [('7968', 9), ('7869', 9), ('813', 8), ('6537', 8), ('6133', 8)]\n",
      "Least repeted elements: [('9945', 1), ('9753', 1), ('9741', 1), ('9604', 1), ('9580', 1)]\n",
      "Unique records 569\n",
      "-------------\n",
      "Using *FR* to group user_id\n",
      "Total records 2298\n",
      "Most repeted elements: [('9252', 10), ('8398', 9), ('1238', 9), ('10648', 9), ('9537', 8)]\n",
      "Least repeted elements: [('9888', 1), ('9875', 1), ('9865', 1), ('9662', 1), ('9570', 1)]\n",
      "Unique records 665\n",
      "-------------\n",
      "Using *IT* to group user_id\n",
      "Total records 1882\n",
      "Most repeted elements: [('4082', 11), ('8430', 9), ('5519', 9), ('3852', 9), ('3283', 9)]\n",
      "Least repeted elements: [('9916', 1), ('9760', 1), ('9640', 1), ('9546', 1), ('9176', 1)]\n",
      "Unique records 528\n",
      "-------------\n",
      "Using *UK* to group user_id\n",
      "Total records 3555\n",
      "Most repeted elements: [('8000', 9), ('7233', 9), ('6518', 9), ('650', 9), ('388', 9)]\n",
      "Least repeted elements: [('9871', 1), ('9796', 1), ('9771', 1), ('9765', 1), ('9659', 1)]\n",
      "Unique records 1043\n",
      "-------------\n",
      "Using *US* to group user_id\n",
      "Total records 3876\n",
      "Most repeted elements: [('4314', 12), ('8075', 9), ('6083', 9), ('5928', 9), ('5893', 9)]\n",
      "Least repeted elements: [('9970', 1), ('9836', 1), ('9722', 1), ('9708', 1), ('9677', 1)]\n",
      "Unique records 1129\n",
      "-------------\n",
      "Different user ids per country: [('UK', 1043), ('DE', 1051), ('IT', 528), ('ES', 569), ('', 792), ('US', 1129), ('FR', 665)]\n",
      "Unique *user_id* per *country*\n",
      "-------------\n",
      "Intersection matrix\n",
      "['UK' 'DE' 'IT' 'ES' '' 'US' 'FR']\n",
      "[1043, 0, 0, 0, 0, 0, 0]UK\n",
      "[0, 1051, 0, 0, 0, 0, 0]DE\n",
      "[0, 0, 528, 0, 0, 0, 0]IT\n",
      "[0, 0, 0, 569, 0, 0, 0]ES\n",
      "[0, 0, 0, 0, 792, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1129, 0]US\n",
      "[0, 0, 0, 0, 0, 0, 665]FR\n"
     ]
    }
   ],
   "source": [
    "from koalas import *\n",
    "\n",
    "base_feature = 'country'\n",
    "base_feature_unique = df_flat[base_feature].unique()\n",
    "\n",
    "print('-----------------------------------------------------------------')\n",
    "print('Base feature *{}* with {} unique values: {}'.format(base_feature, \n",
    "                                                   len(base_feature_unique),\n",
    "                                                   base_feature_unique))\n",
    "print('-----------------------------------------------------------------')\n",
    "\n",
    "# Create quantitative statistics of each group\n",
    "analyze_groups(df_flat, base_feature)\n",
    "\n",
    "secondary_feature = 'user_id' # We'll loop here\n",
    "user_ids_per_country = count_ocurrence_of_one_feature_into_another(df_flat, base_feature, secondary_feature)\n",
    "userids_per_country_counts = [(k,len(val)) for k, val in user_ids_per_country.items()]\n",
    "\n",
    "print('Different user ids per country: ' + str(userids_per_country_counts))\n",
    "print('Unique *{}* per *{}*'.format(secondary_feature, base_feature))\n",
    "\n",
    "print('-------------')\n",
    "\n",
    "print('Intersection matrix')\n",
    "print(base_feature_unique)\n",
    "for country in base_feature_unique:\n",
    "    \n",
    "    print(str([len(user_ids_per_country[country].intersection(user_ids_per_country[c])) for c in df_flat['country'].unique()])\n",
    "         + str(country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have one group of data for each possible conbination of user_id and country. Let's extract how many different user ids per country we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping with: UK\n",
      "Number of samples: 1043\n",
      "Max :9995\n",
      "Min :100\n",
      "Different user ids per country: [('UK', 1043), ('DE', 1051), ('IT', 528), ('ES', 569), ('', 792), ('US', 1129), ('FR', 665)]\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "user_ids_per_country = count_ocurrence_of_one_feature_into_another(df_flat, 'country', 'user_id')\n",
    "country = 'UK'\n",
    "country_group_set = user_ids_per_country[country]\n",
    "# print(country_group_set)\n",
    "print('Grouping with: {}'.format(country))\n",
    "print('Number of samples: {}'.format(len(country_group_set)))\n",
    "country_group_float_list = [float(x) for x in list(country_group_set)]\n",
    "print('Max :{}'.format(max(list(country_group_set))))\n",
    "print('Min :{}'.format(min(list(country_group_set))))\n",
    "# print('Mean :{}'.format(np.mean(country_group_float_list, dtype=np.float64)))\n",
    "\n",
    "\n",
    "\n",
    "userids_per_country_counts = [(k,len(val)) for k, val in user_ids_per_country.items()]\n",
    "\n",
    "print('Different user ids per country: ' + str(userids_per_country_counts))\n",
    "\n",
    "# for country in df_flat['country'].unique():\n",
    "#     print(str(country) + \n",
    "#           str([len(user_ids_per_country[country].intersection(user_ids_per_country[c])) for c in df_flat['country'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Analizing feature: user_id\n",
      "Will use feature :country to group data into 7 groups\n",
      "['UK' 'DE' 'IT' 'ES' '' 'US' 'FR']\n",
      "<class 'set'>\n",
      "Grouping user_id with: UK\n",
      "Number of unique values: 1043\n",
      "Max :9995\n",
      "Min :100\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: DE\n",
      "Number of unique values: 1051\n",
      "Max :9998\n",
      "Min :10\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: IT\n",
      "Number of unique values: 528\n",
      "Max :9916\n",
      "Min :1001\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: ES\n",
      "Number of unique values: 569\n",
      "Max :9994\n",
      "Min :10000\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: \n",
      "Number of unique values: 792\n",
      "Max :9984\n",
      "Min :10019\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: US\n",
      "Number of unique values: 1129\n",
      "Max :9983\n",
      "Min :10005\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping user_id with: FR\n",
      "Number of unique values: 665\n",
      "Max :9987\n",
      "Min :10013\n",
      "--------------\n",
      "----------------------------\n",
      "Analizing feature: country\n",
      "Will use feature :country to group data into 7 groups\n",
      "['UK' 'DE' 'IT' 'ES' '' 'US' 'FR']\n",
      "<class 'set'>\n",
      "Grouping country with: UK\n",
      "Number of unique values: 1\n",
      "Max :UK\n",
      "Min :UK\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: DE\n",
      "Number of unique values: 1\n",
      "Max :DE\n",
      "Min :DE\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: IT\n",
      "Number of unique values: 1\n",
      "Max :IT\n",
      "Min :IT\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: ES\n",
      "Number of unique values: 1\n",
      "Max :ES\n",
      "Min :ES\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: \n",
      "Number of unique values: 1\n",
      "Max :\n",
      "Min :\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: US\n",
      "Number of unique values: 1\n",
      "Max :US\n",
      "Min :US\n",
      "--------------\n",
      "<class 'set'>\n",
      "Grouping country with: FR\n",
      "Number of unique values: 1\n",
      "Max :FR\n",
      "Min :FR\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "def feature_group_analysis(dataframe, feature_base):\n",
    "    count = 0\n",
    "    for feature in dataframe.columns:\n",
    "        print('----------------------------')\n",
    "        print('Analizing feature: {}'.format(feature))\n",
    "        \n",
    "        # List of countries with the unknown one as the first one\n",
    "        unique_countries = df_flat[feature_base].unique()\n",
    "        print('Will use feature :'+ str(feature_base) + ' to group data into ' + str(len(unique_countries)) + ' groups')\n",
    "        print(unique_countries)\n",
    "        df_grouped = df_flat.groupby([feature_base, feature])\n",
    "        \n",
    "        user_ids_per_country = count_ocurrence_of_one_feature_into_another(df_grouped)\n",
    "        \n",
    "        for value in dataframe[feature_base].unique():\n",
    "            country_group_set = user_ids_per_country[value]\n",
    "            print(type(country_group_set))\n",
    "            print('Grouping {} with: {}'.format(feature, value))\n",
    "#             print(df_grouped.get_group(value).shape)\n",
    "            print('Number of unique values: {}'.format(len(country_group_set)))\n",
    "#             country_group_float_list = [float(x) for x in list(country_group_set)]\n",
    "            print('Max :{}'.format(max(list(country_group_set))))\n",
    "            print('Min :{}'.format(min(list(country_group_set))))\n",
    "            print('--------------')\n",
    "#             userids_per_country_counts = [(k,len(val)) for k, val in country_group_set.items()]\n",
    "#             print(userids_per_country_counts)\n",
    "            \n",
    "        count += 1\n",
    "        \n",
    "        if count > 2:\n",
    "            break\n",
    "        \n",
    "feature_group_analysis(df_flat[['user_id', 'country']], 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
